{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Практическое задание\n",
    "## Метод обратного распространения ошибки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### О задании\n",
    "\n",
    "В этом задание вы:\n",
    "* познакомитесь с методом обратного распространения ошибки \n",
    "* реализуете прямой проход и обратный проход в нейросети\n",
    "* реальзуете стохастический градиентный спуск с моментов\n",
    "* обучите нейросеть для классификации рукописных цифр"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Часть 1. Прямой и обратный проход нейросети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Теоретическая часть\n",
    "\n",
    "<p>\n",
    "Метод обратного распространения ошибки — это метод обучения многослойной нейрононной сети, который впервые был открыт двумя независимыми группами исследователей в 1974 г. Этот метод определяет алгоритм эффективного вычисления градиентов параметров нейронной сети, что позволяется применить метод градиентного спуска в задачи минимизации функционала ошибки. \n",
    "</p>\n",
    "\n",
    "Давайте рассморим M-слойную полносвязную нейронную сеть.\n",
    "\n",
    "<img src=\"network.png\" width=\"800\">\n",
    "\n",
    "На рисунке верхний индекс всегда используется для обозначения номера слоя нейросети. Рассмотрим некоторый n-ый слой, который назовем текущим. Данный слой на вход принимает\n",
    "$N^{(n-1)}$, \n",
    "признаков \n",
    "$y^{(n - 1)}_i$, $i=\\overline{1\\mathinner {\\ldotp \\ldotp}N^{(n-1)}}$, \n",
    "которые являются значения выходов нейронов предыдущего слоя, и \n",
    "$y^{(n-1)}_0=1$\n",
    "(смещение или bias нейрона — константа, которая рассматривается как вход нейрона для упрощения записи дальнейших вычислений). Вес нейрона, связывающий $i$'ый нейрон предыдущего слоя с $j$'ым нейроном текущего, обозначен $w^{(n)}_{ij}$. \n",
    "За \n",
    "$z_j^{(n)}=\\sum_{i=0}^{N^{(n-1)}}{w_{ij}^{(n)}y_i^{(n-1)}}$, \n",
    "обозначена линейная комбинация входов и весов.\n",
    "$\\sigma^{(n)}_j$ \n",
    "— функция активации j'ого нейрона(так как функции активации нейронов в общем случае могут быть различны), а \n",
    "$y_j^{(n)} = \\sigma_j^{(n)}(z_j^{(n)}) = \\sigma_j^{(n)}(\\sum_{i=0}^{N^{(n-1)}}{w_{ij}^{(n)}y_i^{(n-1)}}) $ \n",
    "— значение функции активации или выход j'ого нейрона. \n",
    "\n",
    "<img src=\"layer.png\" width=\"800\">\n",
    "\n",
    "Резюмируем обозначения:\n",
    "* $M$ - колличество слоев\n",
    "* $N^{(n)}$ - колличество нейронов в $n$-ом слое\n",
    "* $\\{ w_{ij}^{(n)}\\}$ - веса нейронов $n$-ого слоя\n",
    "* $z_j^{(n)} = \\sum_{i=0}^{N^{(n - 1)}}{w_{ij}^{(n)}y_i^{(n-1)}}$\n",
    "* $\\sigma_j^{(n)}$ - функция активации $j$-ого нейрона $n$-ого слоя\n",
    "* $y_j^{(n)} = \\sigma_j^{(n)}(z_j^{(n)})$ - выход $j$-ого нейрона $n$-ого слоя \n",
    "\n",
    "За $E(\\boldsymbol{\\widehat{y}}, \\boldsymbol{y})$  обозначим некоторую диффенцируюмую функцию ошибки. Здесь  <br/>\n",
    "$\\boldsymbol{\\widehat{y}} = (y_1, ..., y_{N^{(M)}})$ - значение целевой переменной,  \n",
    "$\\boldsymbol{y} = (y_1^{(M)}, ..., y_{N^{(M)}}^{(M)}))$ - выход нейросети. \n",
    "\n",
    "Давайте попробуем оценить сложность вычисление частной производной функции ошибки $E$. Предположим, что сложность вычисления частной производной функции в точке приблизительно равна сложности вычисления самой функции в точке. Пусть нейросеть имеет M полносвязных слоем по N нейронов в каждом слое. Тогда:\n",
    "* $O(N)$ - cложность вычисления одного выхода одного слоя (перемножение N весов на N входов)\n",
    "* $O(N^2)$ - cложность вычисления всех выходов одного слоя\n",
    "* $O(M * N^2)$ - cложность вычисления функции ошибки (последовательно вычисляем выходы M слоев)\n",
    "* $O(M^2 * N^4)$ - cложность частных производных функции ошибки по всем весам (всего  $O(M * N^2)$ весов)\n",
    "\n",
    "Получается для нейросети, состоящей из одного слоя с 1000 нейронами, сложность вычисления градиента должна быть равна $O(10^{12})$. А это уже невероятно много.\n",
    "\n",
    "Идея метода эффективного расчета градиентов заключается в том, чтобы при прямом проходе нейросети сохранить некоторые вычисленные значения, которые потом позволят быстро находить градиент."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вычисление градиента"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Будем вычислять частные производные функции ошибки от последних слоев в первым.\n",
    "Помимо частных производных \n",
    "$$\\frac{\\partial}{\\partial w_{ij}^{(n)}} E$$ \n",
    "будем вычислять значения производных \n",
    "$$\\frac{\\mathrm{\\partial}}{\\partial y_i^{(n)}} E \\quad \\mathrm{и} \\quad \\frac{\\mathrm{\\partial}}{\\partial z_i^{(n)}} E$$ вычисление которых является одним из ключевых моментов в алгоритме обратном распространения ошибки. \n",
    "\n",
    "Полезно посмотреть как аналитически выглядит вычисления выхода нейросети \n",
    "$\\boldsymbol{\\widehat{y}}=(y_1^{(M)}, ..., y_{N^{(M)}}^{(M)})$.:\n",
    "\n",
    "$$ \\widehat{y_j}=y_j^{(M)}=\\sigma_j^{(M)}(\\sum_{i=0}^{N^{(M-1)}}{w_{ij}^{(M)}\\sigma_i^{(M-1)}(\\sum_{k=0}^{N^{(M-2)}}{w_{ki}^{(M-1)}\\sigma_k^{(M-2)}(...)})})$$\n",
    "\n",
    "В случае однослойной нейросети, получим: \n",
    "\n",
    "$$ \\widehat{y_j}=\\sigma_j^{(1)}(\\sum_{i=0}^{N^{(0)}}{w_{ij}^{(1)}x_j}),$$\n",
    "\n",
    "двухслойной - \n",
    "\n",
    "$$ \\widehat{y_j}=\\sigma_j^{(2)}(\\sum_{i=0}^{N^{(1)}}{w_{ij}^{(2)}\\sigma_i^{(1)}(\\sum_{k=0}^{N^{(0)}}{w_{ki}^{(1)}x_k})}).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вычисление градиента начнем с выходного слоя.** \n",
    "\n",
    "Вспомним как вычисляется выход:\n",
    "\n",
    "$$ y_j^{(M)}=\\sigma_j^{(M)}(z_j^{(M)})$$\n",
    "$$z_j^{(M)} = \\sum_{i=0}^{N^{(M-1)}}{w_{ij}^{(M)}y_i^{(M-1)}}$$\n",
    "\n",
    "Будем последовательно вычислять:\n",
    "\n",
    "1. $\\boldsymbol{\\frac{\\partial E}{\\partial y_j^{(M)}}}.$\n",
    "Так как функция E нам известна, то мы можем вычислить частные производные этой функции по переменным \n",
    "$y_1^{(M)}, ..., y_{N^{(M)}}^{(M)}. $ \n",
    "Например, если\n",
    "$$E(\\boldsymbol{\\widehat{y}},  \\boldsymbol{y}^{(M)}) = \\frac{1}{2} \\| \\boldsymbol{\\widehat{y}} - \\boldsymbol{y}^{(M)} \\|_2^2 = \\frac{1}{2} \\sum_{j=1}^{N^{(M)}}{( \\widehat{y_j} - y_j^{(M)}) ^ 2},$$ \n",
    "то\n",
    "$$\\frac{\\partial E}{\\partial y_j^{(M)}} = y_j^{(M)} - \\widehat{y_j}$$\n",
    "Заметим, что в этом случае частная производная $E$ по $y_j^{(M)}$ равна ошибки на объекте $x_i$.\n",
    "\n",
    "2. $\\boldsymbol{\\frac{\\mathrm{\\partial E}}{\\partial z_j^{(M)}}}.$ \n",
    "Функция ошибки \n",
    "$E = E(y_1^{(M)}, ..., y_{N^{(M)}}^{(M)}) = E(y_1^{(M)}(z_1^{(M)}), ..., y_{N^{(M)}}^{(M)}(z_{N^{(M)}}^{(M)}))$.\n",
    "Вычислим \n",
    "$\\frac{\\mathrm{\\partial E}}{\\partial z_j^{(M)}}$ \n",
    "применив правило вычисление производной сложной функции \n",
    "$$\\frac{\\mathrm{\\partial}}{\\partial z_j^{(M)}} E = \\frac{\\partial E}{\\partial y_j^{(M)}} \\frac{\\partial y_j^{(M)}}{\\partial z_j^{(M)}} =  \\frac{\\partial E}{\\partial y_j^{(M)}} (\\sigma^{(M)}_j)' $$\n",
    "Важно, что $(\\sigma^{(M)}_j)'$ берется в точке \n",
    "$$z_j^{(M)}= \\sum_{i=0}^{N^{(M-1)}}{w_{ij}^{(M)}\\sigma_i^{(M-1)}(\\sum_{k=0}^{N^{(M-2)}}{w_{ki}^{(M-1)}\\sigma_k^{(M-2)}(...)})}.$$ \n",
    "Заметим что при прямом проходе, мы  вычисляем значение $z_j^{(M)}$, теперь дополнительно при прямом проходе будет сохранять это значение. Тогда вычисление \n",
    "$\\frac{\\mathrm{\\partial}}{\\partial z_i^{(M)}} E$ \n",
    "представляет собой вычисление значение  $(\\sigma^{(M)}_j)'$ в уже известной точке и перемножение двух чисел.\n",
    "\n",
    "3. $\\boldsymbol{\\frac{\\partial E}{\\partial w_{ij}^{(M)}}}.$ \n",
    "Функция ошибки \n",
    "$E = E(z_1^{(M)}, z_2^{(M)}, ..., z_{N^{(M)}}^{(M)})$. \n",
    "Вспомним, что \n",
    "$z_j^{(M)} = \\sum_{i=0}^{N^{(M-1)}}{w_{ij}^{(M)}y_i^{(M-1)}}$. \n",
    "Вес $w_{ij}^{(M)}$ входит только в одну сумму $z_j^{(M)}$. Тогда:\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial w_{ij}^{(M)}}= \\frac{\\partial E}{\\partial z_j^{(M)}} \\frac{\\partial z_j^{(M)}}{\\partial w_{ij}^{(M)}} =   \\frac{\\partial E}{\\partial z_j^{(M)}} \\frac{\\partial(\\sum_{k=0}^{N^{(M - 1)}}{w_{kj}^{(M)}y_k^{(M - 1)}}) }{\\partial w_{ij}^{(M)}} = \\frac{\\partial E}{\\partial z_j^{(M)}} y_i^{(M - 1)} $$\n",
    "\n",
    "Таким образом, мы  за 3 шага вычислили $\\frac{\\partial}{\\partial w_{ij}^{(n)}} E$ для последнего выходного слоя."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вычисление градиента произвольного внутренного слоя**\n",
    "\n",
    "Рассмотрим произвольный внутренний слой n. Вспомним что выходы этого слоя $y_i^{(n)}$ связаны с $z_j^{(n + 1)}$ следующего слоя соотношением:\n",
    "$$z_j^{(n + 1)}=\\sum_{i=0}^{N_{n}}{w_{ij}^{(n + 1)}y_i^{(n)}}, \\quad  j= \\overline{1\\mathinner {\\ldotp \\ldotp}N^{(n+1)}}$$\n",
    "Можно сказать, что \n",
    "$E = E(z_1^{(n + 1)}, z_2^{(n + 1)},..., z_{N^{(n+1)}}^{(n + 1)})$. \n",
    "А производные \n",
    "$\\frac{\\partial E}{\\partial z_{j}^{(n + 1)}}$ \n",
    "были посчитаны на предыдущем шаге.\n",
    "\n",
    "Тогда для \n",
    "$\\boldsymbol{\\frac{\\partial E}{\\partial y_i^{(n)}}}$ \n",
    "$$\\frac{\\partial E}{\\partial y_i^{(n)}} = \\sum_{j=1}^{N^{(n+1)}}{\\frac{\\partial E}{\\partial z_{j}^{(n + 1)}}} \\frac{\\partial z_{j}^{(n + 1)}}{\\partial y_{i}^{(n)}} = \\sum_{j=1}^{N^{(n+1)}}{\\frac{\\partial E}{\\partial z_{j}^{(n + 1)}}} \\frac{\\partial (\\sum_{k=0}^{N_{n}}{w_{kj}^{(n + 1)}y_k^{(n)}})}{\\partial y_{i}^{(n)}} = \\sum_{j=1}^{N^{(n+1)}}{\\frac{\\partial E}{\\partial z_{j}^{(n + 1)}}} w_{ij}^{(n + 1)} $$\n",
    "\n",
    "Эту величину, по аналогии с последним слоем будем называть ошибкой сети на скрытом слое. Заметим, что \n",
    "$$\\frac{\\partial E}{\\partial y_i^{(n)}} = \\sum_{j=1}^{N^{(n+1)}}{\\frac{\\partial E}{\\partial z_{j}^{(n + 1)}}} w_{ij}^{(n + 1)} =  \\sum_{j=1}^{N^{(n+1)}}{\\frac{\\partial E}{\\partial y_{j}^{(n + 1)}}}(\\sigma^{(n + 1)}_j)'  w_{ij}^{(n + 1)}.$$\n",
    "Таким образом, мы вычисляем ошибку текущего слоя через ошибку предыдущего, распространяя ее \"задом наперед\". Отсюда и название алгоритма — обратное распространение ошибок.\n",
    "\n",
    "Вычисление\n",
    "$\\boldsymbol{\\frac{\\mathrm{\\partial E}}{\\partial z_i^{(n)}}}$ и $\\boldsymbol{\\frac{\\partial E}{\\partial w_{ij}^{(n)}}}$ выполняется абсолютно аналогично последнему слою.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, мы умеем последовательно вычислять частные производные по всем весам от последнего слоя к нейросети к первому. Заметим что все произодные можно быстро вычислять матрично."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Практическая часть"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этой части вам предстоит научиться:\n",
    "* вычислять производные среднеквадратичной функции ошибки и категориальной кросс энтропии\n",
    "* выполнять прямой и обратный проход неросети, состоящей из полносвязных слоей и функции антивации ReLu и Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 1.** Вычисление производных функций ошибок\n",
    "\n",
    "Вам дан интерфейс класса `Loss`, вам нужно реализовать вычисление значение функции и ее градиента для среднеквадратичной ошибки $MSE$, а также для категориальной кросс-энтропии $H$. \n",
    "\n",
    "Пусть $\\widehat{y_i}$ - истинное значение функции, $y_i$ - предсказанное, тогда:\n",
    "$$MSE(\\boldsymbol{\\widehat{y}}, \\boldsymbol{y})=\\frac{1}{d}\\sum_{i=1}^{d}{( \\widehat{y_i} - y_i)^2}$$\n",
    "$$H(\\boldsymbol{\\widehat{y}}, \\boldsymbol{y})=-\\frac{1}{d}\\sum_{i=1}^{d}{\\widehat{y_i}\\log({y_i})}$$\n",
    "\n",
    "Для численной устойчивости вам предлагается также реализовать градиент связки `softax + crossentropy`(в функции `gradient_with_sofmax`). Преобразование $softmax$ над вектором $x=(x_1, ..., x_d)$ можно записать как \n",
    "$$y_i = \\frac{e^{x_i}}{\\sum_{j=0}^{d}{e^{x_j}}}$$\n",
    "Функция `gradient_with_sofmax` должна возвращать \n",
    "$(\\frac{\\partial E}{\\partial x_1}, ..., \\frac{\\partial E}{\\partial x_d})$\n",
    "(Вам необходимо выполнить аналитические преобразования над $\\frac{\\partial E}{\\partial x_i}$ так, чтобы аналичически он зависел только от $\\boldsymbol{\\widehat{y}}$ и $\\boldsymbol{y})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "import base64\n",
    "import copy\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(abc.ABC):\n",
    "    @abc.abstractmethod\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        y_true: np.array(d), ground truth (correct) labels\n",
    "        y_pred: np.array(d), estimated target values\n",
    "        ---\n",
    "        output: loss\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def gradient(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        y_true: np.array(d), ground truth (correct) labels\n",
    "        y_pred: np.array(d), estimated target values\n",
    "        ---\n",
    "        output: np.array(d), gradient loss to y_pred\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanSquaredError(Loss):\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        d = y_true.shape[0]\n",
    "        ans = np.sum(( y_true - y_pred) ** 2 / d)\n",
    "        return ans\n",
    "\n",
    "    def gradient(self, y_true, y_pred):\n",
    "        d = y_true.shape[0]\n",
    "        ans = np.sum(( y_true - y_pred) * 2 / d)\n",
    "        return np.zeros(y_true.shape)\n",
    "\n",
    "\n",
    "class CategoricalCrossentropy(Loss):\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        #your code here\n",
    "        return 0\n",
    "\n",
    "    def gradient(self, y_true, y_pred):\n",
    "        #your code here\n",
    "        return np.zeros(y_true.shape)\n",
    "\n",
    "    def gradient_with_softmax(self, y_true, y_pred):\n",
    "        #your code here\n",
    "        return np.zeros(y_true.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Убедитесь в правильности работы ваших функций с помощью небольшого числа unit-тестов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ERROR_MSG = \"Error in test {}.\\n\\ty_true:{},\\n\\ty_pred:{},\\n\\tactual output:{}\\n\\tdesired output:{}\"\n",
    "\n",
    "def decode_answer(base64_string, shape):\n",
    "    buf = base64.decodebytes(base64_string)\n",
    "    return np.frombuffer(buf, dtype=np.float).reshape(shape)\n",
    "\n",
    "def check_answers(actual_values, desired_values, msg=''):\n",
    "    for i, (actual_value, desired_value) in enumerate(zip(actual_values, desired_values)):\n",
    "        msg = ERROR_MSG.format(i, Y_TRUE[i], Y_PRED[i], actual_value, desired_value)\n",
    "        np.testing.assert_almost_equal(actual_value, desired_value, err_msg=msg, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123456)\n",
    "\n",
    "SHAPE = (10, 5)\n",
    "Y_TRUE = np.rint(np.random.random(size=SHAPE))\n",
    "Y_PRED = np.random.random(size=SHAPE)\n",
    "\n",
    "MSE_ERRORS = decode_answer(\n",
    "    b'I0+rUDZfsj+65BmGzuvTP+ITdyZtp9E/jovoXZbOxj+tM4rNGfjRP8WqZPINQ7o/qkWFqWJrvj/YygOu3+7iP2xXHf8FLtc/nh8u'\n",
    "    b'JeH90z8=', shape=SHAPE[0]\n",
    ")\n",
    "\n",
    "MSE_GRADIENTS = decode_answer(\n",
    "    b'bbbcKhRtpD9tOdGaZi27v2ZYmkJFG3s//n322ZiNyr8m6+LE9aWjP6AI7iZLdsk/C3BaoCJ51T/NU4CxsQalvyCAv50cwdI/Jnla'\n",
    "    b'YuW0uL+ge/FUpbTMP8oHwWfBI8g/078UJnJrrD+agsQdii7Xv/P1Wpvcppe/ALOZhal2qz/J0z1DWiXQPy69kl9AdMK/KyUC2VwB'\n",
    "    b'zb+TKsfV3+qvP61ZjQQ/I6y/861aiGl80T9aw6ZZnY2hP05TbFQuN8a/pf7fNLXD1b91mBtGF3LIP2BQLEj8766/QK1QkZqzrD8A'\n",
    "    b'Z3EKfd/APyILN4pDvMK/IkWU2OF8w7/dKXoh5znEP2Op3oXyxcC/bf/5e0B1s7+ti6ZZedDDP8APmQhD5Ms/m5mMtC1R2T9T4D8Z'\n",
    "    b'LzfZPy+LSRFEHtC/gzVVrZfSzD+G39szoJO5P2acoFCBSoO/d+LLLFDn1D+yZblH9XbYv5OyW97C+8Q/nCtIKLzV2L9dGTmp/Em1'\n",
    "    b'P7SuSAYBJtA//Z/XUtyLwb+9hvZzq4e5Pw==', shape=SHAPE\n",
    ")\n",
    "CROSS_ENTROPY_ERRORS = decode_answer(\n",
    "    b'Lk/El56cyj/WBoxO/q6zP0unis259t4/pUfrxKNr0D96LACShLzgPwpVpmwNtr8/yNI87LnRyz93hYk3LG3JP09lbzfFF+Q/stF9'\n",
    "    b'Vtkm6T8=', shape=(SHAPE[0])\n",
    ")\n",
    "CROSS_ENTROPY_GRADIENTS = decode_answer(\n",
    "    b'AAAAAAAAAICQTFUt2sf1vwAAAAAAAACAmWpZOGWeAMAAAAAAAAAAgAAAAAAAAACAAAAAAAAAAICCQnznotTxvwAAAAAAAACAXyHI'\n",
    "    b'8ogW9b8AAAAAAAAAgAAAAAAAAACAAAAAAAAAAID9kuL7NywlwN9kei0D+/C/AAAAAAAAAIAAAAAAAAAAgNm59BtSBPm/wPXRt3J0'\n",
    "    b'AsAAAAAAAAAAgDhJMLFijPK/AAAAAAAAAIAAAAAAAAAAgIi4e0BwQ/y/9c0zp6WyGsAAAAAAAAAAgHD+3KnZ2PK/AAAAAAAAAIAA'\n",
    "    b'AAAAAAAAgFqtaQ7QO/m/QoDlrBTV+b8AAAAAAAAAgAhQB8ahy/e/DnZxuenA878AAAAAAAAAgAAAAAAAAACAAAAAAAAAAIAAAAAA'\n",
    "    b'AAAAgHRxm/JtmQXAAAAAAAAAAIAAAAAAAAAAgKLsIAbIYvC/AAAAAAAAAIDiZmdoeIw2wAAAAAAAAACA5l+CC9q6QMAAAAAAAAAA'\n",
    "    b'gAAAAAAAAACAtRNb8pFX+L8AAAAAAAAAgA==', shape=SHAPE\n",
    ")\n",
    "CROSS_ENTRY_GRADIENTS_WITH_SOFTMAX = decode_answer(\n",
    "    b'COSTNVmIuT/kw8IgYPzQv0B3oEkL8ZA/vw46iH+Y4L/wpRs2c4+4P8iKqfDd098/DgxxSGvX6j/AaOAdXki6vyhgL8Vjcec/cBfx'\n",
    "    b'uh7izr9E7RZV5/DhP7xJscGxLN4/5PfMVyfDwT9BozWlLPrsv3CzMcKTkK2/4A+A8ykqwT+7SA3UsC7kP3psd3dQEde/O1ehB9og'\n",
    "    b'4r+cepzly/LDPwxY2GIHlsG/cFlx6oPb5T8wdBCwBPG1PyJoh+n5xNu/Tv4XgqI067+SfqIXnY7ePzyyG639VcO/SGzSmkDwwT/A'\n",
    "    b'wA1NXBfVP+rNxGxUa9e/ala5Thpc2L9UtNjpYEjZP7xTVidv99S/SH/4mpBSyL+YLhCwl8TYP9ipX+WpbuE/AsCvIXml7z9o2I/f'\n",
    "    b'+oTvP/vtmxXVJeS/ckFVzJ4D4j9o19JAiPjPP4DDyKQhHZi/Fdv+NyQh6j8ev6eZspTuvzif8pWzOto/gzZaMisL77+0X4fTe5zK'\n",
    "    b'P2Ha2keBL+Q//IeNZ9Pu1b9sKPRQlunPPw==', shape=SHAPE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 1.]\n",
      " [0. 0. 0. 1. 1.]\n",
      " [0. 0. 1. 1. 0.]\n",
      " [1. 0. 0. 1. 1.]\n",
      " [0. 1. 0. 0. 1.]\n",
      " [1. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 1. 0.]\n",
      " [1. 0. 0. 1. 0.]]\n",
      "###\n",
      "[[0.09973676 0.73459622 0.01654451 0.4813845  0.09593887]\n",
      " [0.49730633 0.83879627 0.89733326 0.73259152 0.75872436]\n",
      " [0.56065718 0.47147793 0.13876812 0.09446113 0.94225634]\n",
      " [0.13409924 0.63069955 0.63956822 0.43348979 0.15584706]\n",
      " [0.86260898 0.6830463  0.0857089  0.56610253 0.14982485]\n",
      " [0.47745445 0.84893827 0.14014442 0.32955081 0.63407411]\n",
      " [0.61937849 0.39504264 0.67239782 0.80998032 0.38699906]\n",
      " [0.54475875 0.98894936 0.98498291 0.37038179 0.56294193]\n",
      " [0.2497721  0.97645137 0.81654559 0.04434843 0.40983286]\n",
      " [0.02988663 0.2079005  0.63079895 0.65729823 0.24931601]]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "\nArrays are not almost equal to 7 decimals\nError in test 0.\n\ty_true:[0. 1. 0. 1. 0.],\n\ty_pred:[0.09973676 0.73459622 0.01654451 0.4813845  0.09593887],\n\tactual output:[ 0.03989471 -0.10616151  0.0066178  -0.2074462   0.03837555]\n\tdesired output:[0. 0. 0. 0. 0.]\n(mismatch 100.0%)\n x: array([ 0.0398947, -0.1061615,  0.0066178, -0.2074462,  0.0383755])\n y: array([0., 0., 0., 0., 0.])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-8cbfa64b9678>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmse_gradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mMeanSquaredError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_TRUE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_PRED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mcheck_answers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMSE_GRADIENTS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmse_gradients\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mcross_entropy_errors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCategoricalCrossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_TRUE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_PRED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-d931b1217bd7>\u001b[0m in \u001b[0;36mcheck_answers\u001b[0;34m(actual_values, desired_values, msg)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mactual_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesired_value\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactual_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesired_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mERROR_MSG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_TRUE\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_PRED\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactual_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesired_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_almost_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactual_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesired_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/numpy/testing/nose_tools/utils.py\u001b[0m in \u001b[0;36massert_almost_equal\u001b[0;34m(actual, desired, decimal, err_msg, verbose)\u001b[0m\n\u001b[1;32m    563\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m             \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdesired\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0massert_array_almost_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesired\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecimal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    566\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0;31m# If one of desired/actual is not finite, handle it specially here:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/numpy/testing/nose_tools/utils.py\u001b[0m in \u001b[0;36massert_array_almost_equal\u001b[0;34m(x, y, decimal, err_msg, verbose)\u001b[0m\n\u001b[1;32m    961\u001b[0m     assert_array_compare(compare, x, y, err_msg=err_msg, verbose=verbose,\n\u001b[1;32m    962\u001b[0m              \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Arrays are not almost equal to %d decimals'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdecimal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m              precision=decimal)\n\u001b[0m\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/numpy/testing/nose_tools/utils.py\u001b[0m in \u001b[0;36massert_array_compare\u001b[0;34m(comparison, x, y, err_msg, verbose, header, precision, equal_nan, equal_inf)\u001b[0m\n\u001b[1;32m    777\u001b[0m                                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m                                 names=('x', 'y'), precision=precision)\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: \nArrays are not almost equal to 7 decimals\nError in test 0.\n\ty_true:[0. 1. 0. 1. 0.],\n\ty_pred:[0.09973676 0.73459622 0.01654451 0.4813845  0.09593887],\n\tactual output:[ 0.03989471 -0.10616151  0.0066178  -0.2074462   0.03837555]\n\tdesired output:[0. 0. 0. 0. 0.]\n(mismatch 100.0%)\n x: array([ 0.0398947, -0.1061615,  0.0066178, -0.2074462,  0.0383755])\n y: array([0., 0., 0., 0., 0.])"
     ]
    }
   ],
   "source": [
    "mse_errors = [MeanSquaredError()(y_true, y_pred) for y_true, y_pred in zip(Y_TRUE, Y_PRED)]\n",
    "check_answers(MSE_ERRORS, mse_errors)\n",
    "\n",
    "mse_gradients = [MeanSquaredError().gradient(y_true, y_pred) for y_true, y_pred in zip(Y_TRUE, Y_PRED)]\n",
    "print(mse_gradients)\n",
    "print(\"###\")\n",
    "print(MSE_GRADIENTS)\n",
    "check_answers(MSE_GRADIENTS, mse_gradients)\n",
    "\n",
    "cross_entropy_errors = [CategoricalCrossentropy()(y_true, y_pred) for y_true, y_pred in zip(Y_TRUE, Y_PRED)]\n",
    "check_answers(CROSS_ENTROPY_ERRORS, cross_entropy_errors)\n",
    "\n",
    "cross_entropy_gradients = [CategoricalCrossentropy().gradient(y_true, y_pred) for y_true, y_pred in zip(Y_TRUE, Y_PRED)]\n",
    "check_answers(CROSS_ENTROPY_GRADIENTS, cross_entropy_gradients)\n",
    "\n",
    "cross_entropy_grad_softmax = [CategoricalCrossentropy().gradient_with_softmax(y_true, y_pred) \n",
    "                              for y_true, y_pred in zip(Y_TRUE, Y_PRED)]\n",
    "check_answers(CROSS_ENTRY_GRADIENTS_WITH_SOFTMAX, cross_entropy_grad_softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 2.** Выполнение прямого и обратного прохода нейросети\n",
    "\n",
    "В этой части задания вам будет необходимо реализовать forward и backward проходы для 3-х типов слоев:\n",
    "* `Linear`, выполняет линейную комбинацию входов с весами $y_j=\\sum_{i=0}^{N}{w_{ij}x_i}, j=\\overline{1 {\\ldotp \\ldotp}M}$\n",
    "* `ReLu`, нелинейная активация $y_j^{(n)}=\\max(x_j), j=\\overline{1{\\ldotp \\ldotp}M}$\n",
    "* `Softmax`, $y_j = \\frac{e^{x_j}}{\\sum_{i=0}^{d}{e^{x_i}}}, j=\\overline{1{\\ldotp \\ldotp}M}$\n",
    "\n",
    "Вам дан шаблон нейросетевой модели, которая умееет последовательно добавлять слои друг за другом. Сейчас вам стоит обратит внимание только на функции `__init__`, `add`, `forward` и `backward`. Пока при инициализации модели параметр `optimizer` оставляйте равным None. Также вам дан абстракный класс `Layer`, который предоставляет интерфейс одного слоя нейросети. Вам необходимо реализовать функции `_forward` и `_backward`. Все вычисления необходимо делать матрично.\n",
    "\n",
    "Замечание:\n",
    "    обратите внимание на функцию `_build` слоя `Linear`, веса соответвущие bias'ам добавляются последним столбцом.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self, loss=None, optimizer=None):\n",
    "        self._layers = []\n",
    "        self._loss = loss\n",
    "        self._optimizer = optimizer\n",
    "        self._outputs = None\n",
    "\n",
    "    def add(self, layer):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        if not self._layers:\n",
    "            layer.build(optimizer=self._optimizer)\n",
    "        else:\n",
    "            layer.build(self._layers[-1], optimizer=self._optimizer)\n",
    "        self._layers.append(layer)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = inputs\n",
    "        for layer in self._layers:\n",
    "            outputs = layer.forward(outputs)\n",
    "        self._outputs = outputs\n",
    "        return self._outputs\n",
    "\n",
    "    def backward(self, outputs, use_gradient_softmax_with_loss=False):\n",
    "        if self._loss is None:\n",
    "            raise ValueError(\"Loss is not defined\")\n",
    "\n",
    "        if use_gradient_softmax_with_loss:\n",
    "            grad_outputs = [self._loss.gradient_with_softmax(outputs[i], self._outputs[i])\n",
    "                               for i in range(outputs.shape[0])]\n",
    "            backward_layers = self._layers[:-1]\n",
    "        else:\n",
    "            grad_outputs = [self._loss.gradient(outputs[i], self._outputs[i])\n",
    "                               for i in range(outputs.shape[0])]\n",
    "            backward_layers = self._layers\n",
    "\n",
    "        grad_outputs = np.array(grad_outputs)\n",
    "        for layer in backward_layers[::-1]:\n",
    "            grad_outputs = layer.backward(grad_outputs)\n",
    "\n",
    "    def update_weights(self, x_batch, y_batch, use_gradient_softmax_with_loss=False):\n",
    "        if self._optimizer is None:\n",
    "            raise ValueError(\"Optimizer is not defined\")\n",
    "        self.forward(x_batch)\n",
    "        self.backward(y_batch, use_gradient_softmax_with_loss)\n",
    "        for layer in self._layers[::-1]:\n",
    "            layer.update_weights()\n",
    "\n",
    "    def fit(self, X, Y, batch_size, epochs, shuffle=True, X_val=None, Y_val=None, use_gradient_softmax_with_loss=False):\n",
    "        size = X.shape[0]\n",
    "        X_train, y_train = X[:], Y[:]\n",
    "\n",
    "        self.loss_train_history = []\n",
    "        self.loss_val_history = []\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            if shuffle:\n",
    "                p = np.random.permutation(size)\n",
    "                X_train, y_train = X[p], Y[p]\n",
    "            for step in range(size // batch_size):\n",
    "                ind_slice = slice(step * batch_size, (step + 1) * batch_size)\n",
    "                self.update_weights(X_train[ind_slice], y_train[ind_slice], use_gradient_softmax_with_loss)\n",
    "            train_loss = self.evaluate(X_train, y_train, batch_size)\n",
    "\n",
    "            if (X_val is not None) and (Y_val is not None):\n",
    "                val_loss = self.evaluate(X_val, Y_val, batch_size)\n",
    "                self.loss_val_history.append(val_loss)\n",
    "                self.loss_train_history.append(train_loss)\n",
    "                print(\"Epoch: {:d}, train loss: {:f}, val loss: {:f}\".format(epoch, train_loss, val_loss))\n",
    "            else:\n",
    "                self.loss_train_history.append(train_loss)\n",
    "                print(\"Epoch: {:d}, train loss: {:f}\".format(epoch, train_loss))\n",
    "\n",
    "    def evaluate(self, X, Y, batch_size):\n",
    "        if self._loss is None:\n",
    "            raise ValueError(\"Loss is not defined\")\n",
    "        if X.shape[0] != Y.shape[0]:\n",
    "            raise ValueError(\"X and Y must have equal size\")\n",
    "\n",
    "        Y_pred = np.empty(Y.shape)\n",
    "        size = X.shape[0]\n",
    "        for step in range(size // batch_size + 1):\n",
    "            ind_slice = slice(step * batch_size, (step + 1) * batch_size)\n",
    "            Y_pred[ind_slice] = self.forward(X[ind_slice])\n",
    "        losses = [self._loss(Y[i], Y_pred[i]) for i in range(size)]\n",
    "        return sum(losses) / len(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeInitializer(object):\n",
    "    def __call__(self, shape):\n",
    "        n = shape[0]\n",
    "        return np.random.randn(*shape) * np.sqrt(2.0/n)\n",
    "\n",
    "\n",
    "class ZerosInitializer(object):\n",
    "    def __call__(self, shape):\n",
    "        return np.zeros(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(abc.ABC):\n",
    "    def __init__(self, input_dim=None):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = None\n",
    "        self.inputs = None\n",
    "        self.outputs = None\n",
    "\n",
    "        self.d_inputs = None\n",
    "        self.d_outputs = None\n",
    "        self._optimizer = None\n",
    "\n",
    "        self._is_build = False\n",
    "\n",
    "    def build(self, prev_layer=None, optimizer=None):\n",
    "        self._optimizer = copy.deepcopy(optimizer)\n",
    "        if prev_layer is not None:\n",
    "            self.input_dim = prev_layer.output_dim\n",
    "        elif self.input_dim is None:\n",
    "            raise ValueError('Input dimension is not determine.'\n",
    "                             'If this first layer, please, use param \"input_dim\"')\n",
    "        self._is_build = True\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        if not self._is_build:\n",
    "            raise ValueError(\"Layer is not build\")\n",
    "        if inputs.shape[1:] != (self.input_dim,):\n",
    "            raise ValueError(\"Input shape is not correct\")\n",
    "        return self._forward(inputs)\n",
    "\n",
    "    def backward(self, grad_outputs):\n",
    "        if self.inputs is None:\n",
    "            raise ValueError(\"Forward pass is not performed\")\n",
    "        return self._backward(grad_outputs)\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def _forward(self, inputs):\n",
    "        \"\"\"\n",
    "        inputs: np.array((n, d)), input values, n - batch size, d - number input features\n",
    "        ---\n",
    "        output: np.array((n, c)), output values, n - batch size, c - number output features\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def _backward(self, grad_outputs):\n",
    "        \"\"\"\n",
    "        grad_outputs: np.array((n, c)), gradient by outputs,\n",
    "                      n - batch size, c - number output features of this layer\n",
    "        ---\n",
    "        output: np.array((n, d)), gradient by inputs,\n",
    "                n - batch size,  c - number input features of this layer\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def update_weights(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    def __init__(self, units, input_dim=None,\n",
    "                 weights_initializer=None, bias_initializer=None):\n",
    "        super().__init__(input_dim)\n",
    "        self.output_dim = units\n",
    "\n",
    "        self.weights = None\n",
    "        self.mean_d_weights = None # mean value gradient weights by batch\n",
    "\n",
    "        self._weights_initializer = weights_initializer if weights_initializer else HeInitializer()\n",
    "        self._bias_initializer = bias_initializer if bias_initializer else ZerosInitializer()\n",
    "\n",
    "    def build(self, prev_layer=None, optimizer=None):\n",
    "        super().build(prev_layer, optimizer)\n",
    "        weights = self._weights_initializer((self.input_dim, self.output_dim))\n",
    "        bias = self._bias_initializer((1, self.output_dim))\n",
    "        self.weights = np.vstack((weights, bias))\n",
    "\n",
    "    def _forward(self, inputs):\n",
    "        #your code here\n",
    "        return np.zeros((inputs.shape[0], self.output_dim))\n",
    "\n",
    "    def _backward(self, grad_outputs):\n",
    "        #your code here\n",
    "        self.mean_d_weights = np.zeros(self.weights.shape)\n",
    "        return np.zeros((grad_outputs.shape[0], self.input_dim))\n",
    "\n",
    "    def update_weights(self):\n",
    "        #your code here\n",
    "        pass\n",
    "\n",
    "\n",
    "class ReLU(Layer):\n",
    "    def __init__(self, input_dim=None):\n",
    "        super().__init__(input_dim)\n",
    "\n",
    "    def build(self, prev_layer=None, optimizer=None):\n",
    "        super().build(prev_layer, optimizer)\n",
    "        self.output_dim = self.input_dim\n",
    "\n",
    "    def _forward(self, inputs):\n",
    "        #your code here\n",
    "        return inputs\n",
    "\n",
    "    def _backward(self, grad_outputs):\n",
    "        #your code here\n",
    "        return grad_outputs\n",
    "\n",
    "    def update_weights(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Softmax(Layer):\n",
    "    def __init__(self, input_dim=None):\n",
    "        super().__init__(input_dim)\n",
    "\n",
    "    def build(self, prev_layer=None, optimizer=None):\n",
    "        super().build(prev_layer, optimizer)\n",
    "        self.output_dim = self.input_dim\n",
    "\n",
    "    def _forward(self, inputs):\n",
    "        #your code here\n",
    "        return inputs\n",
    "\n",
    "    def _backward(self, grad_outputs):\n",
    "        #your code here\n",
    "        return grad_outputs\n",
    "\n",
    "    def update_weights(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Убедитесь в правильности реализации вами функций с помощью небольшого числа unit-тестов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_answer(base64_string, shape):\n",
    "    buf = base64.decodebytes(base64_string)\n",
    "    return np.frombuffer(buf, dtype=np.float).reshape(shape)\n",
    "\n",
    "np.random.seed(123456)\n",
    "\n",
    "INPUT_DIM, OUTPUT_DIM = 4, 6\n",
    "BATCH_SIZE = 3\n",
    "COUNT_TESTS = 10\n",
    "\n",
    "INPUTS = np.random.normal(size=(COUNT_TESTS, BATCH_SIZE, INPUT_DIM))\n",
    "WEIGHTS = np.random.normal(size=(COUNT_TESTS, INPUT_DIM + 1, OUTPUT_DIM))\n",
    "LINEAR_D_OUTPUTS = np.random.normal(size=(COUNT_TESTS, BATCH_SIZE, OUTPUT_DIM))\n",
    "RELU_D_OUTPUTS = np.random.normal(size=(COUNT_TESTS, BATCH_SIZE, INPUT_DIM))\n",
    "SOFTMAX_D_OUTPUTS = np.random.normal(size=(COUNT_TESTS, BATCH_SIZE, INPUT_DIM))\n",
    "\n",
    "LINEAR_OUTPUTS = decode_answer(\n",
    "    b'wGm3fs21079DBbbsam/fv0YsX4hX9/o/Nxh4K63n8T++UomsDXPrvxbi2hKA8gjAl03DtDo1AcB05j4D6Bvcv1d2xALzbwBADAx/'\n",
    "    b'H0G0AkD31QxMgaX0P+gv0xPBnfK//f64pm5VBUDLFwHUBV4EwAiYgNzWLA3Ax4deeVK9AsDYJpe26kwEQGVn2t7ZWwLA/HC9xMss'\n",
    "    b'9L+IeOKWVevZv5Rfd295DbI/WHvyPVAW979E6GKRpFPmP8x4zXNcUwTAqCRqOrLM8T/151+zeYr+vx6bEJiWGNC/mhc6p5SqAMCh'\n",
    "    b'SuMGWLDcv24FuPkfPeU//mbyah5d9r+ucwIVIU3oP81dEoOegQVAyKhCIITQDcAwTKAcRY4NQPjHLQNXwALAestiiX2EBkDemn7g'\n",
    "    b'j8DwvzBqu0YGnPw/wNLLm4gz8r8MoZpd8svwv/70sbu0xP2/4Z0Nh94+AMAow1b45oIKwMdHYH/MvwrA506PZHXj4z9aLm5Z2+3s'\n",
    "    b'P8Tk/paM3gZAqCclSm3M+z9pzl5fkHIYQBkKiIdTjRJAkWkBGd358D/IbqQCKN77v7JS+XYenvC/Bo9V1xLnEEAkelfNQC3kPwGY'\n",
    "    b'/nu93/8/sE1Qipm89L/QNoNnBFqkv0TCcC86m+I/tLP+xucZ0j8QA6VJV2HiPyc1Ru5nFgJA5efyWSarA0Cc5Fqc2qrNv2vZQdXk'\n",
    "    b'sQDALOJVE8sv/j/mrk0EYifjP+4ZCcQaJPc/V0d//SUe5b+gPNHw9VzSP/hOIius5gTANiVWAomHFcD7wcLEf3Dwv8EPRV+UIBlA'\n",
    "    b'KHyQtLH1+T9gr+67iXUGQHyz3s0wTNE/1vmK3JcAD8AxuOL3pTL4vwVlfP0a6xdAKk4O0RKRE8DzJk+ETJP1Pwml9+piFgrAsEbz'\n",
    "    b'ANbAC8AYXacpBbXOvxRlJ9bEeOo/yaILuC6LCsDiNBbTRO7Vv6RKEPT8Avu/2uPPVplQ0j+jPgFnam6xvyGX5u0PhPC/X2dnCYQR'\n",
    "    b'EsCgGkfU3kHgvxQ0Am3FNss/V6Y63wHpC8A7tmYislPcPxQ+0M8ZYhFAQIPzNBpF+r9Yv2NY7SoQwOyBsYbZNvI/qLCJjvaL/D9S'\n",
    "    b'O4Mv3CS1vyftMo9OnAvAhP5pHLgYFcAg3GyiFC/pP+R7Q28JJ9C/p96cZCbV8z8pY0LfW6vpv2tGIwTKyxFAmtN1h/cK5T/uR0Ff'\n",
    "    b'LpEEwIOjB57uAfw/iH3ZhwYzB0BipUya/4/pPyz9FjDi5hdARbRzUdzv7j+YYMxaOSILQHANapLprda/IlSg8sQ28D8xDBK2gxnm'\n",
    "    b'P5CW0s+gnbY/gDZeg85Czj9SuLWI8vwLwHZfxhQpCN2/ir6FkxvEEsCShlKitBX2v9rq22/+hvO/QiehO1HX0j/eww3Ft83nP6Vs'\n",
    "    b'dgmGtAZAlP8lO1X99D9QydHZ4nXvPwZiWA6Ssea/TJJIWptA9r/uWjqs06v+v9An3PViyO6/KrQViu7RCcA+Kh7hTMu/v2zoa2ZW'\n",
    "    b'jBXAHrrgqgSM/j8gvM4nNqT3v4xtpLcZYRdAfoxz/N45DsA80sT+DY3yP15FUTkjIPo/oLiF9fzN9T94wKgxQHy9P4DTllkw19O/'\n",
    "    b'MtKPpb+g0b9yPM4rr97iv6xle4TvzP2/0gweWXoCDEBU3w4wKx30P9hmJE5kSAbAJkOcNM34G0BNIxfvXRcTwKb78y1erPe/lJ/Y'\n",
    "    b'redW+L9e47yfGZn1P0ZEwUoRiQHAzIFyAjDGAUC8bw0idVf2vyJwOBRMmAFAL+NuZOhDC8D+5VZNXyoJQLCicm3ysPW/SB7Coj/M'\n",
    "    b'2T8sjM1DuEzrv+rph7ycMhFAPyaGKEi3E8A02vtrHaXqP6L4cDNZe+y/c/Ys2fxu5r8nf4qpwmcSQIi0Q9ysMPI/Fh10yO1sDUAG'\n",
    "    b'6RLr7PUDwG5OmwH+59o/',\n",
    "    shape=(COUNT_TESTS, BATCH_SIZE, OUTPUT_DIM)\n",
    ")\n",
    "LINEAR_D_INPUTS = decode_answer(\n",
    "    b'P2edUgWR+r/bzKvlk3bjP9szpDHiifi/LYE0nMt2779h0J1t7RAMwPm13hRgIvo//0PlFKvn078mY9lXhU7dvzHH2LklFRVAj3BH'\n",
    "    b'TpX7AUDEabx1uEIHwBJTpAf2u9k/2UASQZeD0b8ZiUrHtnD5PzUUC+5tqgHAhKA4PIjR8L98KhExIHv1vyfDZ2OKQPy/z8zIh0tu'\n",
    "    b'DsBRGTYBOEEJwLQRGDDCDRHAq91D5VKlIMD95gLQO6QaQI8ES0rhliJAfZgckVxv9T9G/aTxuLrFvwv4niwXcfg//CNiqtSf67+s'\n",
    "    b'xePaogPqP/8f44x7EwzATjSTlQa21z+fmux6IjwPQGMvrv8zShHAbJQRvmQGAcBPLH4HSQ72vxmsITDMDiNAlDrNEjW99D9csOJ6'\n",
    "    b't7T+P93HoYdoKfM/7k3sVZ6J9r/6iK3y2S7ZP/l1IQvc7QTAFTesB+Nb9L9kIG01nPj/v++DG90/aeA/bOFczA0HAMBKeBXJECcV'\n",
    "    b'QEJNe+3vpBvAnwLL8efe4D+bRDTq7ezfv21YsH6B+vM/L6CZoBR21D8hMDDp4yT8v6XULQufIPY/s7GQkbha+b8exktRol0VQAQM'\n",
    "    b'qBQ/ZeI/qeq5APRYjb+lhC8HfqQCwBQiLbLre/u/0xv/24Gi3r801ygQjJv1P2KwMlh6H/u/NUi9uPw/8z8t3X2eBLfzPxLxpw5f'\n",
    "    b'N+c/RyP/A/ah4j+EobO9K38RwPXhyZcCAQFAVmb/UNjVB8Cd/+qN1o0OwK8yktnHcAjALgt4XYrD+L93CbzwJHv7v/0mhN5VO+u/'\n",
    "    b'SzTIYQ0H4j8Mc01xk1QPQFeOI+NWUQXAsP1mWBWl4r9uhE6QjNMEwGcPimeXce6/yI1g6owq8j9edpTiY4H9v1VBS0rUMwHA3wpn'\n",
    "    b'JKiM+b+gmZUgyMLWP4WXYJ9+lw9A5fhvmrbi5T8tCSJlLwIeQJrtuTutefk/4bkQwFPWEsAgAwRhO+QLwHwagU32TAHA4SMOiYKI'\n",
    "    b'7L+6FngpMMgBQDjYeTf1/Ok/4+2b7LP1+b+gXzZ1qKgMQBElUKRe0tW/cOWiD+H+9T/QarjGuP/8PzmlXoBeww3AIHsffOvw5b9P'\n",
    "    b'4Y9kZCcKwFfPnIHSDwtAwb567vp4wz9AImNVabrQv1ilXYAKTua/yRTFuDpc0b8Rxpcdo6QLQFhZ/XLpVQfAzrFbKSnx8b/4OfW8'\n",
    "    b'ovQFwL295MDX1Oo/pTQXX+ZsAMAKax04xwvwv8jcCidyEgLAYlrJn2I4CUBWSMjMKagOwAkewz9Jnum/',\n",
    "    shape=(COUNT_TESTS,  BATCH_SIZE, INPUT_DIM)\n",
    ")\n",
    "LINEAR_MEAD_D_WEIGHTS = decode_answer(\n",
    "    b'arfqHIAf479QsLlR0m7Wv+jN7ldX6+y/BpChF3zO4r+iO2bPKy/kv/2bdU6oisS/DiFD39Ma1L99QrcdZLrNv0KzC2HEb/K/n35H'\n",
    "    b'WJ2J3b83YGFa7N7kPyNAC1DDoqS/+CC4vvv+zj9Das+5yKinv5tyGRSeGMI/OWWympzx07+Ftb6M3CfgPxMqUKMHYOk/Nb1MLbVm'\n",
    "    b'6D+MCqeZckvXP0VSDrTOPvM/QNo7F1dP4D9TTSLALTPmPxnZ6L6n8uE/3Eoyzo780r+fv7SODo61v9CWDJI137k/4DEvqKkblb+7'\n",
    "    b'URIg42vxv2kD6okB3NS/1AllJjKQ6z98C8dIGr95P81bRcd3OMa/BFQe+lkr4L+/6OnoQdOjPzF9zjIkkK4/FIdNlRa5yb8VCkvn'\n",
    "    b'xhHBv9Uf8azEtL8/ynoNTAMDoj9/VXgneSTDv6jl+qMRrsq/d2DyQa0S+j8ZYBX9Kd3Wv3PPiSI6+bC/hABDe4iF67+1RrhSNQnX'\n",
    "    b'v4wdMNwj5tW/7ADbBBAx47+VPRR0msPVPzEC2dIDlrO/S3U26I2L6T9w8uvHkfbTP9VwiTbq7+A/J46sCQnu8L8r2SLRrSZ8v26W'\n",
    "    b'ATJ6fMM/i2TxxobCsL8doERHZ/aiP8NirrEgmtC/Y/YOGLrk5b+tous4Fn/Tv4hZTNPnst6/QTwB4nVetb+/ehcpxL3HP/EEdNKq'\n",
    "    b'lKs/aCawyDkg2T9VvYpYnG6nvw9Oou01DM0/B26MkVIuwj9VcuoOy1/wv+aK6ClOqdS/GZdYPeuC7L9PJw3O2YPnv0yt1binSPW/'\n",
    "    b'AvgQvByj47/p2QORQK/bP5kxgHJOquq/Vdq8oh/F6j+j8vUqhWvaPzIRwuMfwOA/DCCeqHZohj99+R1SI2S/PwoU1KtTfLC/YGIZ'\n",
    "    b'rKge77/DRA+HjlO3v9P2TkRFW9O/GBq7h6bRqD9b951ij0jtP0vtuEvb9uc/EXFrNq2E0D+D5QuRJ7LaP4OWI2iSLtK/8VhmW9yj'\n",
    "    b'6z+dFaRn5u7YP/PEW9U1tdy/bPa3t4Bz078x2BNPJKPRv4tp66+QyMc/SVuo3FgN1L8J3jF1nFGaP3TnWOkLt+c/2p2TlN2b5L8o'\n",
    "    b'wi8r7ofUvxs0iRHaQuQ/91Qlr9zt5L8MdWywPQGvv7Qm8jXTfec/Yz3Vr3y41r9ZP+2CQdbEv8rYDpBmOqA/l1NCINeT0z8JH8QE'\n",
    "    b'xXXgP76jPbRvg/M/oSJ5KdU81D8VQK3fVCyeP2vMKOhM56U/BZzrQA065L+Y7tDchoDmv0octzYo2PK/09Tl2se20L+VR9p6w6ng'\n",
    "    b'Py97+8+YdO8/GY2Ne+aDxT//8cj2z4qnvyWuZEad4fU//Jk/MSbXxL+8SsKLs3LgPxx0RhYjCu8/rOc0IsdnuT9Il9BhEieov3dg'\n",
    "    b'rP6KYPM/32QVYgu+0z+tURTx01/evxOE+J83UOm/SOaIvyUquL98UvdfsyO1P3Djfkny2/a/b+oz3DbMwb/U+UYpwcjVP3wfVR7E'\n",
    "    b'0Hk/odno8Hwq4r9XLF9XhDrSv6mFfNKLPPU/8HTl7HDNqr8P80KavPfePzXrvspu5+k/cQNiV0l3tb+biTHuj2u2v/MKMn9aIPE/'\n",
    "    b'i4Sjw58d5z8JlSifqwagv3cTV0MnJsm/zwNNPCVt8z/4vZrAbNHVP+gMHM9KvNo/KaoVfEMGAcCPuG9bwtziv8QXBlKdu+4/QzGT'\n",
    "    b'GegB+b9lyoyc+1XoP+8L5L0RquO/ud2z2b/s8D8eEsYVvIjiP1vPVU+/Cey/vFGN6Z9Y5r9vdHeb1az1v9H53gnWF72/gAvQjsoI'\n",
    "    b'AMCPnUx6VQnnv+NlhZ7DnPE/NC7dg5i/5r/HnktUju3zP3f4u/xaCde//W+EBL/C8D9h+aILAbbgP0OyioqCqey/WWjYgB0R5r9z'\n",
    "    b'c30eDzz0vwTjxye+sbq/ShLZt0cwsz8ovJGv+SHkP4Vl9csAA7C/rpcmJq4Z5b+EfET8lCvHP26w3mw9f4S/M9Cb/myByr+XP+IA'\n",
    "    b'c4XevwmdPXKelpo/3aoZilJwzj/5E1EgvB7Avwkbzzl1q6u/G4Wj3MoP5T+FnN3skvfmP8XZHeUb/ZM/59vxTkuRwb8jL7TU7aTR'\n",
    "    b'P1XrcMOdsrk/aWPI3zX0tb+3s6jTXT3Yv9mqBKsh2K0/uJXE+4YR6b9MJvsDFPbLP/Gx8J+Vdte/mX7Xh41b3T/D+uSIDyTlv4DA'\n",
    "    b'qy2aRcQ/tdjMwp6p7j/ldmmKBnqhvxVipc8j1Y8/kY6ZUT/82j90Z6cf+vKfv/0jJMafAtg/t8+tW7Tbvj+454FNAAimv1kdWjA2'\n",
    "    b'XOi/CzZ2TdEo279o8RYchjq/P0OZtGmgxtO/JezwgRUfyL83blLkT2yhP/knW//60uo/1fEmQXrO5b/2+0nprcHRv3vLkB5zzt6/'\n",
    "    b'19DiGucd3T/M79EZ2qXmv63cP7s/kdU/MSsrLva51b+MhISLNzjdv7Kw9U8d0eC/geJJfPp72j/zlqT5CIXMv7fb8xOe1IO/bkbV'\n",
    "    b'wBEO0T8lJPnG377gv+o7CHnhleC/1Qhtj/ZJpT+IJwjf7srqPwG3ZFVGus+/H3VAagNd1L/AxlnIbfr2vzYGDaCWLtK/B5QBlpJZ'\n",
    "    b'9b/BhKXLoaXjv/x0heWipOi/Sem05AlN5b/fK/5MeT7ev/XH6nI1a9K/28QbyMIu4r+cuOK2SWXyvwSSD7LmPcm/4cdGYiw62r80'\n",
    "    b'ZFYQs/0DwKmgrBRly9a/4J3ru6g3BcAcQAUQu8b2v2C1VwG7Pue/7swLWfwq0z+rJCsbpIPNv0MB99PE9sY/P2TEcWzp4b/oRKlM'\n",
    "    b'/JXKv5+O0sNMe+I/qz3oRYwh6L94gghIDufvv8xcrZb619i/nABHslxf8L/bB/6O4I30v+t/YsOD0uC/bF58i6Iv3L8B+DE7Wsb3'\n",
    "    b'P1/XDYzjKOO/CvVrQwC/4b+zkB8mNBndP6N8te73c4w/LaByY+YuvD++sT3s1H7Qvzz04N5Lesu/YnFoVUVdkr85E4jMjraXvxGb'\n",
    "    b'kuucnJq/dUkB69gLtj9fm91p0qDVvzQrCT8+oNM/V7k2cw/vuz/PbjpBIAi3v8mUtCcw/dK/O6P/YvXzxD9yZgOdPh3iv34KQUqg'\n",
    "    b'adM/atEm4hTJxD9EZC3renrCv4JwkSmHAtO/X2JvdLjZ4j+RROs4lg38v30HXm6IBck/a3HT1JvmxT9tLRZWofnRv6x/rrVosvO/',\n",
    "    shape=(COUNT_TESTS, INPUT_DIM + 1, OUTPUT_DIM)\n",
    ")\n",
    "RELU_OUTPUTS = decode_answer(\n",
    "    b'YiyQmO8F3j8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACg2CiUz2TzPwAAAAAAAAAA5CtrTHaEvj8AAAAAAAAAAAAAAAAAAAAAAAAA'\n",
    "    b'AAAAAAAAAAAAAAAAADJCr78bJvE/WS8D2voW5z8AAAAAAAAAAAAAAAAAAAAAaQxgASdm0T8AAAAAAAAAADv4UNwHJeI/V/k5EMmt'\n",
    "    b'0T8AAAAAAAAAAAAAAAAAAAAAIy7b6g8YvT8AAAAAAAAAAKTzoe+yzOA/ISOwt7Dm2T81IbokKXfiPwAAAAAAAAAAAAAAAAAAAAAA'\n",
    "    b'AAAAAAAAAAAAAAAAAAAAAAAAAAAAAACSemmQTAnrPy1VcWBaNvE/AAAAAAAAAACF50HKCEz6PwAAAAAAAAAAwd68x2zZ1j8AAAAA'\n",
    "    b'AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD6952Oveto/vCtLVNO00T8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA'\n",
    "    b'AAAAAAAAAAAAXxHvVLep7D85r0oej8TpPwAAAAAAAAAAt63DYXGGBEDB+lOtbOb2P/bXRrHncfU/AAAAAAAAAAAAAAAAAAAAAJBN'\n",
    "    b'ZdUcS9o/pBgdwQ8L6j8av8TZeuXAPwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAHkBh1wAFfI/AAAAAAAAAAAAAAAAAAAAAAaRztEK'\n",
    "    b'uvk/2udSvApj8D/gzG3RNDriPwujSd1sB+w/AAAAAAAAAACoQyp30y7vPwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABgTDylweOE/'\n",
    "    b'AAAAAAAAAAAAAAAAAAAAAAzBuD87oug/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACEltIyyUPmP2Oqm8b53tU/iBCS'\n",
    "    b'chK27j8AAAAAAAAAAAAAAAAAAAAAtdJt8fMqwz8AAAAAAAAAAIi+ZPDzAeY/DW95bLqVxj/pR+S/0s/ZPwAAAAAAAAAAPraYpdBN'\n",
    "    b'0z8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAVSfL4M2f3PwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAHGHHIgYB/U/9MSA1TkZ5j+m'\n",
    "    b'K3z4Rd3vP33JHTebLANAKp/kCql0jj8w2YC2AtwKQAAAAAAAAAAAAAAAAAAAAACmls8Rbq3sPwAAAAAAAAAAAAAAAAAAAAAAAAAA'\n",
    "    b'AAAAACSn9qtmWNg/oXMhsVm4tT9KDGAkRqzbPwY2K17MUfg/AAAAAAAAAABnNImjpzTjP5ENrHP7jNE/',\n",
    "    shape=(COUNT_TESTS, BATCH_SIZE, INPUT_DIM)\n",
    ")\n",
    "RELU_D_INPUTS = decode_answer(\n",
    "    b'A2itoRO18L8AAAAAAAAAgAAAAAAAAACAAAAAAAAAAIAHh/vZZPO/PwAAAAAAAACAWRIg0o5y1b8AAAAAAAAAgAAAAAAAAACAAAAA'\n",
    "    b'AAAAAAAAAAAAAAAAABE1RhH4o8o/KgQl1O5e5L8AAAAAAAAAAAAAAAAAAACAuuBfgdfy3T8AAAAAAAAAAOFo1PooyfQ/kqsXtUZd'\n",
    "    b'7z8AAAAAAAAAgAAAAAAAAACA0Ev2eU0CzD8AAAAAAAAAgDOyhn98d+k/RvVD7zz+sj8PYlvxi/bPvwAAAAAAAAAAAAAAAAAAAIAA'\n",
    "    b'AAAAAAAAAAAAAAAAAACAAAAAAAAAAAAS5p+ceXSxP4Kfa3xtPuk/AAAAAAAAAICRu8tJJc/tPwAAAAAAAACAVNcnv2uq8r8AAAAA'\n",
    "    b'AAAAgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgFqConRSR+M/qWsKZIi7+b8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIAAAAAAAAAA'\n",
    "    b'AAAAAAAAAAAARSt0hcOH1L+esNsN7Yn+PwAAAAAAAACAez+0wK/A3j8Itz0J1tjvv/ivP2za98a/AAAAAAAAAIAAAAAAAAAAAMVB'\n",
    "    b'rpc6rOk/hzNsPsw04D/jrFsZmwX6PwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgFLdAtr9fvi/AAAAAAAAAIAAAAAAAAAAAJkqln2f'\n",
    "    b'2t0/IChXD6IWwL+9Ou0Wx5z1Pz3sQ2fQrvO/AAAAAAAAAIDiIBDEYqMCQAAAAAAAAACAAAAAAAAAAIAAAAAAAAAAANEfbPk1BKY/'\n",
    "    b'AAAAAAAAAAAAAAAAAAAAAEvJuYm5Gt4/AAAAAAAAAIAAAAAAAAAAAAAAAAAAAACAAAAAAAAAAAB9zMZQP/PJP8BXEKpVDag/O7Yp'\n",
    "    b'MAiSwD8AAAAAAAAAAAAAAAAAAACApRbFTUOw6b8AAAAAAAAAAAG3xBTkt+y/vEFvt/tC/T+ejuHmq+/jvwAAAAAAAAAAKMW0BHXh'\n",
    "    b'1T8AAAAAAAAAgAAAAAAAAACAAAAAAAAAAICXmOxlXy7nPwAAAAAAAACAAAAAAAAAAAAAAAAAAAAAgPAWWwNPlIS/i9lVufSUz78O'\n",
    "    b'xmPOGt72v8/IkgrUPvS/3AyaWWXA5j9JW3lZsi7qvwAAAAAAAAAAAAAAAAAAAADWcSkNqkPUvwAAAAAAAACAAAAAAAAAAIAAAAAA'\n",
    "    b'AAAAAC6UQFNDQOy/SSAyS6fH/b8/A8h1B8Lnv1G0bA9o/v6/AAAAAAAAAIBUspOra+zrP+v3rx0Rmvi/',\n",
    "    shape=(COUNT_TESTS,  BATCH_SIZE, INPUT_DIM)\n",
    ")\n",
    "SOFTMAX_OUTPUTS = decode_answer(\n",
    "    b'xXhAbjms4T+kt8Giv6nQPzVFkyFmjrM/DBZi4c9ovD+9/2O0t+7iP44M7/No88I/U3O850ljyT+nBBJLubmvPxT87vyvibo/okfs'\n",
    "    b'spSinj/Z0qx5mibDP4qJX7wO8OY/l5LFH5o63z9Thj+aifG9P8BtGLOBd7U/ZXDkDCPr0z/IwiZOFI3EP5s57yGFtds/TWOx06O3'\n",
    "    b'1D/UBjCNMzG1PygFI9W5Y8I/d2S+q7Q01D+XoIyKpHKwP83wDEfFfN4/TZGKROcn2T/Z7S4nI+PdPxWaCFi/Kag/ZbYVpfa+tz/j'\n",
    "    b'y3AyoZfIPwqapYg9YrY/+uXC6quTsj8GvfaEWrvkP7TEueeHU9Q/U4IzYvzcuD9ssTWdee7hPxF+31RchJk/+H6EohVj4j+p1hFy'\n",
    "    b'9jbKP2yoaJx9abE/RtmnNfSHwz+EAnm6CdCyP3NM4EVc9dk/Phc5Ih2i1j9dt5BSCGnFP/e/vb1UidQ/etZ55g/8zD+ImpUDiLLU'\n",
    "    b'P4x035Y2jMA/EQnOG8JrwT82R0KU99O/P+R3sWe3B5E/FqlGy1Ii5z/OMhEaAWPdP0XDudD/1No/itb+kyNvoT9zvNSKami2PwTx'\n",
    "    b'EuhvENI/Gp3wsa8H2z/usO6aUlbLP6RlFGLc8rQ/puBG0pL5yT+xPT3zjBmxP2jJA2Dbs+U/oWst0OSoqj+hqzDFonKZP4UIrPFZ'\n",
    "    b'UuA/WIhcsW810j/Gt/B9ZB3HP5yWGOW8Bd0/WroVVtIvlT/YhpTupwPgP7UA0IVh/5k/orWs6WSIxz+sf3qzlWPQP3KcH9Yqm94/'\n",
    "    b'TiQ+BjT0tD/0gvjjYly3PyWmjDoWgeU/RA5ViJ4ftj/SnqZfpj3DP16af68gyMU/6Wf0OUhMxj/U69O12KfYPwkTctXyTdE/IWc2'\n",
    "    b'XM0B4j9ShSGuPi2yP3R+u6ZNrr0/z/BbMoIF0D+4D+jmodq3Pwm+Bal1rNg/IvxNqzOYzT/5P5kHyJDSP8mZuU3jR9U/d73rJi3M'\n",
    "    b'4D8+PRtUOHmmP4IOrmdtQrk/wpkNSXWMsj/UJG9Abf7pP8SFlAqk2qA/vfwurs4StT8zl5ubCdmzPyHFqQjqFdo/duHz1n72yz9X'\n",
    "    b'ZPUklPjSP8wdLsKQ99A/YYHty7gTmT/vJlfHtCvmP/PDSyai/JE/67D3fQJksT8qxg5hUlbiP3Ws7SgtYsI/amLbE4iSyz9NPV2T'\n",
    "    b'XXKbP4+nZp+yQ9Y/s4VBEkSR0D/p/iF143PXP4B2/H6DleE/eUUAMLjHsj+sRXJKPgnMP5q9m6HXPMQ/',\n",
    "    shape=(COUNT_TESTS, BATCH_SIZE, INPUT_DIM)\n",
    ")\n",
    "SOFTMAX_D_INPUTS = decode_answer(\n",
    "    b'WIS5wPxs4T/tYtAttQTgv9cCAzg36bG/Et7nguqbmj+RCJAVjLXIPzxW5KylgX2/ipaHs5Vswr9MPYXSpHOlvw2PEG2zW7y/JwA4'\n",
    "    b'6NU4ab+dTU/lBrmwP+ACBm7m2Kg/B/NST+5g3r9C2RHZsTzIP/yg5DkmurI/T7yhqBcoyz9vVDiNFadzPwpePGQmasi/iCAwIL87'\n",
    "    b'wT+C64leukSqPyixnj8Xe8I/k4hfLUpN0j9E/V2gBuGyPz0wo7qLIeC/r/IQwL1l3D/X0+Wf2/zhvxO113eJf7E/0j0mDrmgqT8m'\n",
    "    b'x0KV81DSP+DxONaMWbA/16nv9zr3tT8U7syIJeXbv6+7H1CZlbW/+coU9Og+vz+ZgoyXbui0v5nmLuc9fqY/dXkJ2kNvxL9McPuv'\n",
    "    b'eP7UP9BQ3+dJ66s/WTvl/3+IzL/IA9At1d6yv0xMvOWEcMo/2sbh6jB90b9LQ++GR/nBP/9ZUvw3m9s/GAO8VhNR3r/POg6w6VjA'\n",
    "    b'Pz3RdfZl2rW/qtEOh/cbuj+msDlAwpKyv2ydqxZGD6K/ANsKSNzmdz+QFp++o3vTPzcOtL/SJ9a/OQROdZy5ir9EPvsl3w+sP/ib'\n",
    "    b'Zs5/AsI/B6RNRqRM1D8Q8otoA47cv17+n54Y/Ie/rhdKMI9awT9+ILwcdYOuv8SymrvdwLi/aE6Spec1lT/4QPSzqMCevxRLlmqf'\n",
    "    b'Hsw/NqirnEao0b/0Gn/KBRS2Pz0YL5M80dO/jlzx7lGWgr/hgPyMrDDTP9wjolopVJM/NsC9m/sZur+zRdRP4eW3PypKj8jrO4+/'\n",
    "    b'Go/tE19umD9VAUaMLreoP73ULAeK0bw/jxu5zPBKsz99eAQNCTzOv2omI7+oHtW/OOU1Jclfx79YIGD5GNykP/oUkjIKM94/dtv2'\n",
    "    b'Bq3v2z+NWtKJvHm0v355k/lF6sW/MBBxzzW4x78PMyqAB/iYP/TdmjfU6am/gKn04JArmT+A9W3hAAFbPzf5e97n9bG/aDwuGZW5'\n",
    "    b'vL/eZWZDPuORv4vnQUQmlMk/sNL224W1mD9wTPCp6MeJPwyOtgLSt7c/IiZ5RxiPwL+Rrgnnpyinv3MTNHvFG8M/iGt/imN+nD8/'\n",
    "    b'lcHyZ+HAv31Hy8mFneG/gyw+x8uBkL80oAPJPw7jP1jqzB90lZ2/04p+0+MRnD/qQP8J1fvFv20rlcJDLJy//hTiByH/xT/qnEBS'\n",
    "    b'KCJ/P/B+GpViZ5C/eIHg7ViN07/0Jul1RhfUP8KBvfGu5bS/tIGJ1Kx4eD/V5GOKk927P2T23cve/qC/',\n",
    "    shape=(COUNT_TESTS,  BATCH_SIZE, INPUT_DIM)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ERROR_MSG = \"Error in test {}:\\ntrue values:{},\\ndesired_values:{}\"\n",
    "\n",
    "def check_answers(actual_values, desired_values, msg=''):\n",
    "    np.testing.assert_almost_equal(actual_values, desired_values, err_msg=msg, verbose=False)\n",
    "\n",
    "def test_forward(layer, true_outputs, set_weights=False, **kwargs):\n",
    "    layer.build()\n",
    "    for i, (input, true_output) in enumerate(zip(INPUTS, true_outputs)):\n",
    "        if set_weights:\n",
    "            layer.weights = WEIGHTS[i]\n",
    "        desired_output = layer.forward(input)\n",
    "        msg = ERROR_MSG.format(i, true_output, desired_output)\n",
    "        check_answers(true_output, desired_output, msg)\n",
    "        \n",
    "def test_backward(layer, true_d_inputs, d_outputs, true_mean_d_weights=None, **kwargs):\n",
    "    layer.build()\n",
    "    for i, (input, d_output, true_d_input) in enumerate(zip(INPUTS, d_outputs, true_d_inputs)):\n",
    "        if true_mean_d_weights is not None:\n",
    "            layer.weights = WEIGHTS[i]\n",
    "        layer.forward(input)\n",
    "        desired_d_input = layer.backward(d_output)\n",
    "        check_answers(true_d_input, desired_d_input, \n",
    "                      msg=ERROR_MSG.format(i, true_d_input, desired_d_input))\n",
    "        if true_mean_d_weights is not None:\n",
    "            check_answers(true_mean_d_weights[i], layer.mean_d_weights, \n",
    "                          msg=ERROR_MSG.format(i, true_mean_d_weights[i], layer.mean_d_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_forward(Linear(OUTPUT_DIM, INPUT_DIM), LINEAR_OUTPUTS, set_weights=True)\n",
    "test_backward(Linear(OUTPUT_DIM, INPUT_DIM), LINEAR_D_INPUTS, LINEAR_D_OUTPUTS, LINEAR_MEAD_D_WEIGHTS)\n",
    "\n",
    "test_forward(ReLU(INPUT_DIM), RELU_OUTPUTS, set_weights=True)\n",
    "test_backward(ReLU(INPUT_DIM), RELU_D_INPUTS, RELU_D_OUTPUTS)\n",
    "\n",
    "test_forward(Softmax(INPUT_DIM), SOFTMAX_OUTPUTS, set_weights=True)\n",
    "test_backward(Softmax(INPUT_DIM), SOFTMAX_D_INPUTS, SOFTMAX_D_OUTPUTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 2. Обучение нейросети стохастическим градиентным спуском"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Теоретическая часть \n",
    "#### Градиентный спуск\n",
    "Настройка весов нейросети может быть сведена к поиску вектора $w$, доставляющего минимум функционалу ошибки:\n",
    "\n",
    "$$E(w) = E( \\boldsymbol{y},  \\boldsymbol{\\widehat{y}}) \\to min $$\n",
    "\n",
    "Минимизировать E(w) будем с помощью градиентного спуска. Правило изменения вектора весов на каждой итерации: \n",
    "\n",
    "$$\n",
    "w^{(k)} = w^{(k - 1)} - \\beta \\nabla_w E(w^{(k - 1)})\n",
    "$$\n",
    "Длину шага $\\beta > 0$ в рамках данного задания предлагается брать равной некоторой малой константе.\n",
    "\n",
    "В случае полного градиентного спуска $\\nabla_w E(w)$ считается все объекты выборки). В случае градиентного спуска  с минибатчем \n",
    "$$\\nabla_w E(w) \\approx \\frac{1}{n}\\sum_{j=1}^{n}{\\nabla_w q_{i_{k_j}} (w)}$$\n",
    "где $q_{i_{k_j}}$ — случайно выбранные номера слагаемых, а n меньше общего колличества примеров для обучения.\n",
    "\n",
    "#### Момент импульса(momentum)\n",
    "Может оказаться, что направление антиградиента сильно меняется от шага к шагу. Чтобы добиться болле эффективной сходимости, можно усреднять векторы антиградиента с нескольких предыдущих шагов — в этом случае шум уменьшится, и такой средний вектор будет указывать в сторону общего направления движения. Введем вектор инерции:\n",
    "    $$h_0 = 0;$$\n",
    "    $$h_k = \\alpha h_{k - 1} + \\beta \\nabla_w E(w^{(k - 1)}) $$\n",
    "Тогда шаг градиентного спуска будет:\n",
    "    $$w^{(k)} = w^{(k - 1)}  - h_k$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Практическая часть\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 3.** Реализация стохастического градиентного спуска с моментом\n",
    "\n",
    "\n",
    "Вам необходимо реализовать алгоритм обновления весов с моментом. Для этого необходимо реализовать метод `update_weights` класса `SGD`. Также вам необходимо реализовать метод `update_weights` во всех слоях нейросети(у которых есть веса)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(object):\n",
    "    def __init__(self, lr, momentum=0):\n",
    "        self._lr = lr\n",
    "        self._momentum = momentum\n",
    "\n",
    "    def update_weights(self, weights, gradient):\n",
    "        \"\"\"\n",
    "        weights: np.array((n, m)), current weigths of algorithm\n",
    "        gradient: np.array((n, m)), average gradient by weights\n",
    "        ---\n",
    "        output: np.array((n, m)), new weights values\n",
    "        \"\"\"\n",
    "        \n",
    "        #your code here\n",
    "        \n",
    "        return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 4.** Обучение нейросети для задачи классификации цифр mnist\n",
    "\n",
    "В этой части вам необходимо обучить вашу нейросеть для задачи классификации рукописных цифр mnist. Вам потребуются методы `fit` и `evaluate` класса `Model`. Можете использовать предложенную архитектуру или выбрать любую другую. Шаг обучения, количество эпох и размер батча выбирите на ваше усмотрение. Посчитайте ошибку и точность(accuracy) предсказания на тестовом датасете.\n",
    "\n",
    "Обучите нейросеть без момента и c моментом(выбранным на ваше усмотрение) и постройте графики ошибки во время обучения в зависимости от числа итераций. Постройте те же графики для валидационной части. Сделайте выводы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mnist\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(images, labels):\n",
    "    binarizer = LabelBinarizer()\n",
    "    X = images.reshape((images.shape[0], 28 * 28))\n",
    "    y = binarizer.fit_transform(labels)\n",
    "    return X / 255.0, y \n",
    "\n",
    "def create_mnist_model(loss, optimizer):\n",
    "    model = Model(loss=loss, optimizer=optimizer)\n",
    "    model.add(Linear(10, input_dim=28*28))\n",
    "    model.add(ReLU())\n",
    "    model.add(Linear(10))\n",
    "    model.add(Softmax())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = prepare_data(mnist.train_images(), mnist.train_labels())\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.33)\n",
    "X_test, y_test = prepare_data(mnist.test_images(), mnist.test_labels())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_mnist_model(model, loss=CategoricalCrossentropy(), optimizer=SGD(lr=0.01))\n",
    "\n",
    "#your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
